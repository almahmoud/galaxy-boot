---

- name: Decide on password
  set_fact:
    rancher_password: "{{ random_pwd }}"
    using_random_pwd: true
  when: rancher_password == ''

- debug:
    msg: "Using rancher pwd {{ rancher_password }}"
    verbosity: 2

- name: Pull and run the Rancher/server container
  docker_container:
      name: "{{ rancher_name }}"
      image: rancher/rancher:{{ rancher_version }}
      restart_policy: unless-stopped
      ports:
        - "{{ rancher_port_80}}:80"
        - "{{ rancher_port }}:443"

- debug: msg="Using rancher_server address {{ rancher_server }}"

- name: Setup login
  uri:
    url: "https://{{ rancher_server }}:{{ rancher_port }}/v3-public/localProviders/local?action=login"
    method: POST
    body_format: json
    body:
      username: admin
      password: admin
    validate_certs: no
    status_code: 201
    return_content: yes
  ignore_errors: yes
  register: login_response
  until: login_response['status'] | default(0) == 201
  retries: 5
  delay: 10

- name: Set current admin password
  set_fact:
    rpwd: admin
    token: "{{ login_response.json.token }}"
  when: login_response is succeeded

- name: Re-setup login
  uri:
    url: "https://{{ rancher_server }}:{{ rancher_port }}/v3-public/localProviders/local?action=login"
    method: POST
    body_format: json
    body:
      username: admin
      password: "{{ rancher_password }}"
    validate_certs: no
    status_code: 201
    return_content: yes
  register: login_response
  when: login_response is failed

- name: Set current admin password
  set_fact:
    rpwd: "{{ rancher_password }}"
    token: "{{ login_response.json.token }}"
  when: rpwd is undefined

- name: Change password
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/users?action=changepassword
    method: POST
    body_format: json
    body:
      currentPassword: "{{ rpwd }}"
      newPassword: "{{ rancher_password }}"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no

# Not used but left in case we need it one day
# - name: Create an API key
#   uri:
#     url: https://{{ rancher_server }}:{{ rancher_port }}/v3/token
#     method: POST
#     body_format: json
#     body:
#       type: token
#       description: cloudman-boot ansible
#       ttl: 3600000  # Expires in 1 hour
#     headers:
#       Authorization: Bearer {{ token }}
#     validate_certs: no
#     status_code: 201
#   register: api_token

- name: Configure server URL
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/settings/server-url
    method: PUT
    body_format: json
    body:
      name: server-url
      type: setting
      value: "https://{{ rancher_server }}:{{ rancher_port }}"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no

- name: Check if cluster already exists
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/clusters?name={{ cm_cluster_name }}
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
  register: clusters

- name: Store existing cluster id
  set_fact:
    cluster_id: "{{ clusters.json.data[0].id }}"
  when: clusters.json.data | length > 0

- name: Create a (cloud) cluster
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/cluster
    method: POST
    body_format: json
    body:
      type: cluster
      name: "{{ cm_cluster_name }}"
      rancherKubernetesEngineConfig:
        ignoreDockerVersion: true
        ingress:
          provider: nginx
          type: ingressConfig
        cloudProvider:
          name: "{{ kube_cloud_provider }}"
          customCloudProvider: |-
            {{ kube_cloud_conf | b64decode }}
        services:
          type: "/v3/schemas/rkeConfigServices"
          kubeApi:
            type: "/v3/schemas/kubeAPIService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
          kubeController:
            type: "/v3/schemas/kubeControllerService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
              # for GCP: https://github.com/projectcalico/canal/issues/74
              configure-cloud-routes: "{{ true if kube_cloud_provider == 'gce' else false }}"
          kubelet:
            type: "/v3/schemas/kubeletService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
          kubeproxy:
            type: "/v3/schemas/kubeproxyService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
          scheduler:
            type: "/v3/schemas/schedulerService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  register: cluster
  when: not cluster_id is defined and kube_cloud_provider

- name: Store new cloud cluster id
  set_fact:
    cluster_id: "{{ cluster.json.id }}"
  when: cluster is not skipped

- name: Create a (local) cluster
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/cluster
    method: POST
    body_format: json
    body:
      type: cluster
      name: "{{ cm_cluster_name }}"
      rancherKubernetesEngineConfig:
        ignoreDockerVersion: true
        ingress:
          provider: nginx
          type: ingressConfig
        services:
          type: "/v3/schemas/rkeConfigServices"
          kubeApi:
            type: "/v3/schemas/kubeAPIService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
          kubeController:
            type: "/v3/schemas/kubeControllerService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
          kubelet:
            type: "/v3/schemas/kubeletService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
          kubeproxy:
            type: "/v3/schemas/kubeproxyService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
          scheduler:
            type: "/v3/schemas/schedulerService"
            extraArgs:
              feature-gates: "TTLAfterFinished=true"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  register: cluster
  when: not cluster_id is defined and not kube_cloud_provider

- name: Store new local cluster id
  set_fact:
    cluster_id: "{{ cluster.json.id }}"
  when: cluster is not skipped

# Special handling for AWS
# https://rancher.com/docs/rke/latest/en/config-options/cloud-providers/aws#tagging-amazon-resources
# There is a possible race condition here if the cluster initialization completes before the tag
# is set, which seems unlikely. However, in such an event, we would have to decompose setting the
# cloudProvider into two steps. First create the cluster without the cloudProvider configured,
# then, obtain the cluster id and set the instance tag and finally update the cluster again with
# the correct cloudProvider configured.
- name: Set aws instance tag
  script: set_aws_instance_tag.py {{ cluster_id }}
  environment:
    CM_INITIAL_CLUSTER_DATA: "{{ cm_initial_cluster_data }}"
  when: kube_cloud_provider == "aws"

- name: Generate cluster registration token and extract node command
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/clusterregistrationtoken
    method: POST
    body_format: json
    body:
      type: clusterRegistrationToken
      clusterId: "{{ cluster_id }}"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  register: node_command

- name: Extract node command
  set_fact:
    node_command: "{{ node_command.json.nodeCommand }}"

- name: Remove sudo from the node command
  set_fact:
    node_command: "{{ node_command | replace('sudo ', '') }}"

- debug: msg="Node cmd is {{ node_command }} {{ node_command }} {{ rancher_master_etcd }} {{ rancher_master_controlplane }} {{ rancher_master_as_worker }}"

- name: Get current cluster status
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/clusters/{{ cluster_id }}
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
  register: cluster_state1

- name: Add master as a node
  command: "{{ node_command }} {{ rancher_master_etcd }} {{ rancher_master_controlplane }} {{ rancher_master_as_worker }}"
  when: cluster_state1.json.state == "provisioning"

- name: Wait for the cluster to become active
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/clusters/{{ cluster_id }}
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
  register: cluster_state_dbg

- name: Wait for the cluster to become active
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/clusters/{{ cluster_id }}
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
  register: cluster_state2
  until: '"active" in cluster_state2.json.state'
  retries: 50
  delay: 15

- name: Add provider block storage class
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port}}/v3/clusters/{{ cluster_id }}/storageclass
    method: POST
    body_format: json
    body:
      type: "storageClass"
      name: "ebs-provisioner"
      reclaimPolicy: "Delete"
      allowVolumeExpansion: true
      provisioner: "kubernetes.io/aws-ebs"
      parameters:
        type: "gp2"
        encrypted: "false"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  when: kube_cloud_provider == "aws"

- name: Add provider block storage class
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port}}/v3/clusters/{{ cluster_id }}/storageclass
    method: POST
    body_format: json
    body:
      type: "storageClass"
      name: "ebs-provisioner"
      reclaimPolicy: "Delete"
      allowVolumeExpansion: true
      provisioner: "kubernetes.io/azure-disk"
      parameters:
        kind: "shared"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  when: kube_cloud_provider == "azure"

- name: Add provider block storage class
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port}}/v3/clusters/{{ cluster_id }}/storageclass
    method: POST
    body_format: json
    body:
      type: "storageClass"
      name: "ebs-provisioner"
      reclaimPolicy: "Delete"
      allowVolumeExpansion: true
      provisioner: "kubernetes.io/gce-pd"
      parameters:
        type: "pd-ssd"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  when: kube_cloud_provider == "gce"

- name: Add provider block storage class
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port}}/v3/clusters/{{ cluster_id }}/storageclass
    method: POST
    body_format: json
    body:
      type: "storageClass"
      name: "ebs-provisioner"
      reclaimPolicy: "Delete"
      allowVolumeExpansion: true
      provisioner: "kubernetes.io/cinder"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  when: kube_cloud_provider == "openstack"

- name: Check if cloudman app is in the catalog already
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/catalogs?name={{ cm_catalog_name }}
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
  register: cm_helm_app

- name: Add custom Helm repos to the Rancher catalog
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/catalog
    method: POST
    body_format: json
    body:
      branch: "{{ item.branch }}"
      kind: helm
      name: "{{ item.app_name }}"
      type: catalog
      url: "{{ item.app_url }}"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  when: cm_helm_app.json.data | length == 0
  with_items:
    - { app_name: "{{ galaxy_catalog_name }}", app_url: 'https://github.com/CloudVE/helm-charts.git', branch: 'master' }
  loop_control:  # Rancher's concurrency here does not keep up without sleep
    pause: 10

- name: Get project info
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/projects?clusterId={{ cluster_id }}
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
  register: project

- name: Set project id
  set_fact:
    project_id: "{{ project.json.data[0].id }}"
    system_project_id: "{{ project.json.data[1].id }}"

- name: Check if CloudMan namespace already exists
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/cluster/{{ cluster_id }}/namespaces?name={{ cm_namespace_name }}
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
  register: cm_ns

- name: Add CloudMan namespace
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/clusters/{{ cluster_id }}/namespace
    method: POST
    body_format: json
    body:
      name: "{{ cm_namespace_name }}"
      type: namespace
      projectId: "{{ project_id }}"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  when: cm_ns.json.data | length == 0

- name: Add kube-system namespace to the Default project
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/cluster/{{ cluster_id }}/namespaces/kube-system
    method: PUT
    body_format: json
    body:
      baseType: namespace
      id: kube-system
      name: kube-system
      projectId: "{{ project_id }}"
      type: namespace
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no

- name: Fetch kubectl config from Rancher
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/clusters/{{ cluster_id }}?action=generateKubeconfig
    method: POST
    headers:
      Content-Type: application/json
      Authorization: Bearer {{ token }}
    validate_certs: False
  register: kubectl_config

- name: Make sure ~/.kube dir exist
  file:
    path: ~/.kube
    state: directory

- name: Save kubectl config
  copy:
    content: "{{ kubectl_config.json.config }}"
    dest: ~/.kube/config

- name: Setup service account
  shell: /usr/local/bin/kubectl create serviceaccount --namespace kube-system tiller && /usr/local/bin/kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

- name: Init Helm
  shell: /usr/local/bin/helm init --service-account tiller

- name: Wait for tiller pod to start...
  command: /usr/local/bin/helm list
  register: tiller_ready
  until: tiller_ready.rc == 0
  retries: 30
  delay: 2

- name: Add CloudVE Helm repo
  shell: /usr/local/bin/helm repo add galaxyproject https://raw.githubusercontent.com/cloudve/helm-charts/master/

- name: Update Helm repos
  shell: /usr/local/bin/helm repo update

- name: Fallback to hostPath provisioner when no cloud provider is set
  shell: /usr/local/bin/kubectl apply -f /tmp/cm-boot/roles/rancher/files/hostPath_storageClass.yaml
  when: not kube_cloud_provider

# Persistent Volumes are being added instead above
- name: Check if nfs-provisioner app has been added
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/project/{{ project_id }}/apps?name=nfs-provisioner
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
  register: nfs_app

- name: Add nfs-provisioner app
  uri:
    url: https://{{ rancher_server }}:{{ rancher_port }}/v3/projects/{{ project_id }}/app
    method: POST
    body_format: json
    body:
      name: "nfs-provisioner"
      targetNamespace: "{{ cm_namespace_name }}"
      type: "app"
      prune: false
      projectId: "{{ project_id }}"
      externalId: "catalog://?catalog=library&template=nfs-provisioner&version=0.2.2"
      answers:
        defaultImage: "true"
        image.repository: "quay.io/kubernetes_incubator/nfs-provisioner"
        image.tag: "v1.0.9"
        persistence.enabled: "true"
        persistence.hostPath: "/nfs"
        persistence.size: "{{ cm_initial_storage_size }}"
        persistence.storageClass: "ebs-provisioner"
        storageClass.allowVolumeExpansion: "true"
        storageClass.create: "true"
        storageClass.defaultClass: "true"
        storageClass.reclaimPolicy: "Delete"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  when: nfs_app.json.data | length == 0

- name: Add Kubernetes Dashboard app
  uri:
    url: https://{{ rancher_server }}:4430/v3/projects/{{ system_project_id }}/app
    method: POST
    body_format: json
    body:
      name: "kubernetes-dashboard"
      targetNamespace: "kube-system"
      type: "app"
      prune: false
      projectId: "{{ system_project_id }}"
      externalId: "catalog://?catalog=library&template=kubernetes-dashboard&version=1.2.0"
      answers:
        defaultImage: "true"
        image.repository: "rancher/kubernetes-dashboard-amd64"
        image.tag: "v1.10.1"
        enableSkipLogin: "false"
        rbac.clusterAdminRole: "true"
    headers:
      Authorization: Bearer {{ token }}
    validate_certs: no
    status_code: 201
  ignore_errors: true

- name: Place Galaxy chart custom values file
  copy:
    src: gxy_chart_values.yml.j2
    dest: /tmp/gxy_chart_values.yml

- name: Helm install CVMFS-CSI
  command: >
    /usr/local/bin/helm install galaxyproject/galaxy-cvmfs-csi --namespace=cvmfs

- name: Helm install Galaxy
  command: >
    /usr/local/bin/helm install galaxyproject/galaxy
    -f /tmp/gxy_chart_values.yml

- name: Wait for Galaxy to become accessible
  uri:
    url: "https://{{ rancher_server }}/galaxy"
    method: GET
    validate_certs: no
  register: cm_available
  until: cm_available['status']|default(0) == 200
  retries: 160
  delay: 10
  when: cm_skip_cloudman is not defined or not (cm_skip_cloudman|bool)

- name: System help info
  debug:
    msg: |
      "The system has now been setup. Access CloudMan at https://{{ rancher_server }}/"
      ""
      "There are also these additional services available:"
      "  Rancher: https://{{ rancher_server }}:{{ rancher_port }}/"
      "  Galaxy: https://{{ rancher_server }}/galaxy/"
      ""
      "{{ svc_access_line }}"
      ""
      "For documentation and information about these services, see http://cloudve.org/"
...
